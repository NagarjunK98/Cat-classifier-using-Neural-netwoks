{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imporing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading input images from folder\n",
    "image_list=[]\n",
    "for i in range(1,500):\n",
    "    img=cv2.imread(\"C:/Users/nagarjun/Desktop/datset/trail(cat)/cat.\"+str(i)+\".jpg\")\n",
    "    resized_image=cv2.resize(img,(64,64))\n",
    "    image_list.append(resized_image)\n",
    "for i in range(1,500):\n",
    "    img=cv2.imread(\"C:/Users/nagarjun/Desktop/datset/trail(dog)/dog.\"+str(i)+\".jpg\")\n",
    "    resized_image=cv2.resize(img,(64,64))\n",
    "    image_list.append(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting list of image date to numpy array\n",
    "X=np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating target column that is Y\n",
    "Y=[1 for i in range(0,499)]\n",
    "for i in range(0,499):\n",
    "    Y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list to numpy array and reshaping \n",
    "Y=np.array(Y)\n",
    "Y=Y.reshape((len(Y),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting up of input data to train and test data\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.1,random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(898, 64, 64, 3)\n",
      "(898, 1)\n",
      "(100, 64, 64, 3)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "#printing shape of train and test data\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transposing train and test data to fit to the model\n",
    "X_train=X_train.T\n",
    "X_test=X_test.T\n",
    "Y_train=Y_train.T\n",
    "Y_test=Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 64, 64, 898)\n",
      "(1, 898)\n",
      "(3, 64, 64, 100)\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "#After transposing the train and test data\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convent reshaping the image data to a flatten data\n",
    "X_train=X_train.reshape((64*64*3,X_train.shape[3]))\n",
    "X_test=X_test.reshape((64*64*3,X_test.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 898)\n",
      "(1, 898)\n",
      "(12288, 100)\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "# After flattening of data \n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to standardize the data\n",
    "X_train=X_train/255.\n",
    "X_test=X_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification activation function\n",
    "def sigmoid(z):\n",
    "    a=1/(1+np.exp(-z))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializeing the input, hidden and , output layers\n",
    "def layer_sizes(X, Y):\n",
    "    n_x = X.shape[0]\n",
    "    n_h1 = 10\n",
    "    n_h2=10\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    return (n_x, n_h1,n_h2, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is: n_x = 12288\n",
      "The size of the hidden layer 1 is: n_h = 10\n",
      "The size of the hidden layer 2 is: n_h = 10\n",
      "The size of the output layer is: n_y = 1\n"
     ]
    }
   ],
   "source": [
    "(n_x, n_h1,n_h2, n_y) = layer_sizes(X_train, Y_train)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer 1 is: n_h = \" + str(n_h1))\n",
    "print(\"The size of the hidden layer 2 is: n_h = \" + str(n_h2))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the weights randomly and bias to zero by considering shape of each layer\n",
    "def initialize_parameters(n_x, n_h1,n_h2, n_y):\n",
    "    W1 =np.random.randn(n_h1,n_x)\n",
    "    W1=W1/100\n",
    "    b1 =np.zeros(shape=(n_h1,1))\n",
    "    W2 =np.random.randn(n_h2,n_h1)\n",
    "    W2=W2/100\n",
    "    b2 =np.zeros(shape=(n_h2,1))\n",
    "    W3 = np.random.randn(n_y,n_h2)\n",
    "    W3=W3/100\n",
    "    b3 = np.zeros(shape=(n_y,1))\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\":W3,\n",
    "                  \"b3\":b3\n",
    "                 }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.00212122  0.00057178 -0.00742828 ... -0.00614358 -0.00645455\n",
      "  -0.00353545]\n",
      " [-0.00888065 -0.00298556 -0.00430568 ...  0.00771289  0.00314298\n",
      "   0.01008381]\n",
      " [ 0.00559851  0.01492402 -0.00169116 ...  0.00926967  0.00808662\n",
      "  -0.00784013]\n",
      " ...\n",
      " [-0.0034015  -0.00865336  0.00100751 ... -0.01090226 -0.00414439\n",
      "   0.00626191]\n",
      " [-0.0119882   0.01071509 -0.0071056  ... -0.00144498 -0.00092474\n",
      "  -0.00418934]\n",
      " [ 0.01921997 -0.00744919 -0.00298948 ... -0.00274046  0.01243944\n",
      "  -0.00798288]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.00453243  0.02334634  0.00596356 -0.01015623 -0.00161819  0.00824381\n",
      "   0.00893739  0.00679375  0.00249501  0.01534315]\n",
      " [-0.01744768 -0.00599153  0.00297099 -0.00975569 -0.00459325 -0.005324\n",
      "   0.00629438 -0.00297468 -0.01108322 -0.01248622]\n",
      " [ 0.00434055 -0.00875919  0.00029159 -0.00025792  0.02007209  0.00201123\n",
      "   0.01483394 -0.00189709 -0.0090318  -0.00353304]\n",
      " [-0.00934368 -0.00123546 -0.00019848  0.01347554  0.0014232   0.00503808\n",
      "   0.01686061 -0.0116459  -0.01629545  0.00141571]\n",
      " [-0.00378576 -0.02121863  0.00353816 -0.00891926  0.00606813 -0.00641725\n",
      "   0.00198478  0.00885957 -0.01728809 -0.00809507]\n",
      " [-0.00692646  0.01186914 -0.02598367  0.01096142  0.00785581  0.00486051\n",
      "   0.00271528 -0.0114542  -0.0070494   0.01114467]\n",
      " [-0.0107432   0.00198763 -0.01442694  0.01113324 -0.00172009 -0.00160082\n",
      "  -0.01135997  0.00691255  0.01343315  0.00195081]\n",
      " [ 0.00860403  0.00941057  0.00287987  0.00550004 -0.0014486  -0.0144758\n",
      "  -0.00839271 -0.01882713  0.00419539 -0.00259448]\n",
      " [-0.00114602 -0.00224115  0.00721208 -0.00399295  0.00385886  0.01368528\n",
      "   0.00014944  0.00942675 -0.00775537  0.00263179]\n",
      " [ 0.02434846 -0.02105996 -0.0054228  -0.0087561   0.01220086 -0.01466741\n",
      "  -0.01082609 -0.00705423  0.00771261  0.00642891]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W3 = [[ 0.00824368 -0.00143737 -0.01674142 -0.00583866  0.02452778  0.00835228\n",
      "  -0.00820384  0.00540602 -0.00331012  0.00502619]]\n",
      "b3 = [[0.]]\n",
      "12.235685657929043\n",
      "0.010119365788289529\n",
      "0.0011885095335384564\n"
     ]
    }
   ],
   "source": [
    "parameters =initialize_parameters(X_train.shape[0], 10, 10, Y_train.shape[0])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "print(\"b3 = \" + str(parameters[\"b3\"]))\n",
    "print(np.power(parameters['W1'],2).sum())\n",
    "print(np.power(parameters['W2'],2).sum())\n",
    "print(np.power(parameters['W3'],2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = (10, 12288)\n",
      "b1 = (10, 1)\n",
      "W2 = (10, 10)\n",
      "b2 = (10, 1)\n",
      "W3 = (1, 10)\n",
      "b3 = (1, 1)\n"
     ]
    }
   ],
   "source": [
    "#printing shape of weight and bias arrays\n",
    "print(\"W1 = \" + str(parameters[\"W1\"].shape))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"].shape))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"].shape))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"].shape))\n",
    "print(\"W3 = \" + str(parameters[\"W3\"].shape))\n",
    "print(\"b3 = \" + str(parameters[\"b3\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation to compute each layer output and given that as input to next layer\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = np.tanh(Z2)\n",
    "    Z3=np.dot(W3,A2)+b3\n",
    "    A3=sigmoid(Z3)\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2,\n",
    "             \"Z3\":Z3,\n",
    "             \"A3\":A3\n",
    "            }\n",
    "    \n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3, cache = forward_propagation(X_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing cost by considering last layer output from forward propagation\n",
    "def compute_cost(A3, Y, parameters,lamb):\n",
    "    m = Y.shape[1] \n",
    "    logprobs = np.multiply(np.log(A3),Y)+np.multiply((1-Y),np.log(1-A3))\n",
    "    l2_re=np.power(parameters['W1'],2).sum()+np.power(parameters['W2'],2).sum()+np.power(parameters['W3'],2).sum()\n",
    "    l2_sum=(lamb/(2*m))*l2_re\n",
    "    cost= (-np.sum(logprobs)/m)\n",
    "    cost = float(np.squeeze(cost)) \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost =  0.6931498280538929\n"
     ]
    }
   ],
   "source": [
    "cost=compute_cost(A3,Y_train,parameters,0.4)\n",
    "print(\"cost = \",cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation used to compute gradients of each layers weights and bias\n",
    "def backward_propagation(parameters, cache, X, Y,lamb):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    A3 = cache['A3']\n",
    "    dZ3=A3-Y\n",
    "    dW3=(1/m)*np.dot(dZ3,A2.T)+((lamb/m)*parameters['W3'].sum())\n",
    "    db3 = (1/m)*np.sum(dZ3,axis=1,keepdims=True) \n",
    "    dZ2 = np.multiply(np.dot(W3.T,dZ3),(1-np.power(A2,2)))\n",
    "    dW2 = (1/m)*np.dot(dZ2,A1.T)+((lamb/m)*parameters['W2'].sum())\n",
    "    db2 = (1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T,dZ2),(1-np.power(A1,2)))\n",
    "    dW1 = (1/m)*np.dot(dZ1,X.T)+((lamb/m)*parameters['W1'].sum())\n",
    "    db1 = (1/m)*np.sum(dZ1,axis=1,keepdims=True)    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2,\n",
    "             \"dW3\":dW3,\n",
    "             \"db3\":db3\n",
    "            }\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.00073537 0.00073526 0.00073541 ... 0.00073474 0.00073479 0.00073478]\n",
      " [0.00073466 0.00073472 0.00073467 ... 0.00073495 0.00073496 0.00073496]\n",
      " [0.00073481 0.00073481 0.00073481 ... 0.00073482 0.00073482 0.00073482]\n",
      " ...\n",
      " [0.00073491 0.00073488 0.00073492 ... 0.00073481 0.0007348  0.0007348 ]\n",
      " [0.00073439 0.00073454 0.00073433 ... 0.00073501 0.00073498 0.00073496]\n",
      " [0.00073471 0.0007346  0.00073466 ... 0.00073453 0.00073449 0.0007345 ]]\n",
      "db1 = [[ 2.84645936e-07]\n",
      " [-7.40396673e-08]\n",
      " [ 1.23860143e-09]\n",
      " [ 6.43505270e-07]\n",
      " [-1.07131871e-07]\n",
      " [-5.79914141e-07]\n",
      " [-1.25869181e-07]\n",
      " [ 2.33253327e-07]\n",
      " [-3.78953056e-07]\n",
      " [-2.48047993e-07]]\n",
      "dW2 = [[ 6.80600810e-06 -8.90170275e-06  1.26979851e-05 -6.98833384e-05\n",
      "  -2.39813507e-05  1.15351753e-05 -3.81760672e-05 -2.98743876e-07\n",
      "  -2.94264989e-05 -6.30919839e-05]\n",
      " [-1.73025489e-05 -1.45649891e-05 -1.83298266e-05 -3.92738224e-06\n",
      "  -1.19323980e-05 -1.81273437e-05 -9.45840523e-06 -1.60639307e-05\n",
      "  -1.09827581e-05 -5.11295547e-06]\n",
      " [-5.54150758e-05 -2.35309110e-05 -6.73847611e-05  1.00360734e-04\n",
      "   7.11854291e-06 -6.50280320e-05  3.59432481e-05 -4.09968829e-05\n",
      "   1.81798342e-05  8.65599234e-05]\n",
      " [-2.82552319e-05 -1.71355369e-05 -3.24229519e-05  2.60580305e-05\n",
      "  -6.45408132e-06 -3.16127250e-05  3.58984164e-06 -2.32257144e-05\n",
      "  -2.59429472e-06  2.12423282e-05]\n",
      " [ 4.73733258e-05  6.70576262e-07  6.49083240e-05 -1.80860448e-04\n",
      "  -4.42611629e-05  6.14541168e-05 -8.64820903e-05  2.62459225e-05\n",
      "  -6.04621119e-05 -1.60645776e-04]\n",
      " [ 7.02952344e-06 -8.84895971e-06  1.29931669e-05 -7.05671043e-05\n",
      "  -2.40947157e-05  1.18133401e-05 -3.84645229e-05 -1.29888686e-07\n",
      "  -2.96282137e-05 -6.36771647e-05]\n",
      " [-3.41454489e-05 -1.85223045e-05 -3.99996388e-05  4.21635110e-05\n",
      "  -3.50723810e-06 -3.88539763e-05  1.06232262e-05 -2.70751803e-05\n",
      "   1.90422233e-06  3.53912435e-05]\n",
      " [-2.57737383e-07 -1.05540004e-05  3.60637131e-06 -5.05605441e-05\n",
      "  -2.04523412e-05  2.84485302e-06 -2.97599318e-05 -4.91659592e-06\n",
      "  -2.40251536e-05 -4.61032944e-05]\n",
      " [-2.19683501e-05 -1.56616687e-05 -2.43307437e-05  8.82717923e-06\n",
      "  -9.60166114e-06 -2.38630183e-05 -3.90278450e-06 -1.91139523e-05\n",
      "  -7.41460291e-06  6.10193532e-06]\n",
      " [-1.20098309e-06 -1.07803115e-05  2.38942233e-06 -4.79554442e-05\n",
      "  -1.99733544e-05  1.66891076e-06 -2.86329525e-05 -5.53376299e-06\n",
      "  -2.32960407e-05 -4.38211047e-05]]\n",
      "db2 = [[ 1.83795403e-05]\n",
      " [-3.20566736e-06]\n",
      " [-3.73349239e-05]\n",
      " [-1.30041325e-05]\n",
      " [ 5.47177256e-05]\n",
      " [ 1.85515746e-05]\n",
      " [-1.82722588e-05]\n",
      " [ 1.20561144e-05]\n",
      " [-7.37934222e-06]\n",
      " [ 1.12052056e-05]]\n",
      "dW2 = [[ 3.38186380e-05  9.61898333e-05 -1.38267949e-05 -1.43842249e-04\n",
      "   1.18371671e-04 -2.37752243e-04 -1.37426843e-04 -3.67424682e-05\n",
      "   1.02585514e-04  3.22193933e-06]]\n",
      "db2 = [[0.00223176]]\n"
     ]
    }
   ],
   "source": [
    "grads = backward_propagation(parameters, cache, X_train, Y_train,lamb=0.3)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print (\"db2 = \"+ str(grads[\"db2\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW3\"]))\n",
    "print (\"db2 = \"+ str(grads[\"db3\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the the weights and bias by considering learning rate and gradients\n",
    "def update_parameters(parameters, grads,alpha):\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    dW3 = grads['dW3']\n",
    "    db3 = grads['db3']\n",
    " \n",
    "    W1 = W1-alpha*dW1\n",
    "    b1 = b1-alpha*db1\n",
    "    W2 = W2-alpha*dW2\n",
    "    b2 = b2-alpha*db2\n",
    "    W3 = W3-alpha*dW3\n",
    "    b3 = b3-alpha*db3\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\":W3,\n",
    "                  \"b3\":b3\n",
    "                 }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.00211902  0.00056958 -0.00743049 ... -0.00614579 -0.00645675\n",
      "  -0.00353765]\n",
      " [-0.00888285 -0.00298776 -0.00430789 ...  0.00771069  0.00314078\n",
      "   0.01008161]\n",
      " [ 0.0055963   0.01492182 -0.00169336 ...  0.00926746  0.00808442\n",
      "  -0.00784234]\n",
      " ...\n",
      " [-0.00340371 -0.00865556  0.0010053  ... -0.01090447 -0.0041466\n",
      "   0.0062597 ]\n",
      " [-0.01199041  0.01071289 -0.00710781 ... -0.00144718 -0.00092695\n",
      "  -0.00419154]\n",
      " [ 0.01921777 -0.00745139 -0.00299168 ... -0.00274267  0.01243723\n",
      "  -0.00798508]]\n",
      "b1 = [[-8.53937807e-10]\n",
      " [ 2.22119002e-10]\n",
      " [-3.71580428e-12]\n",
      " [-1.93051581e-09]\n",
      " [ 3.21395613e-10]\n",
      " [ 1.73974242e-09]\n",
      " [ 3.77607542e-10]\n",
      " [-6.99759982e-10]\n",
      " [ 1.13685917e-09]\n",
      " [ 7.44143980e-10]]\n",
      "W2 = [[ 0.00453241  0.02334637  0.00596352 -0.01015602 -0.00161812  0.00824378\n",
      "   0.00893751  0.00679375  0.0024951   0.01534334]\n",
      " [-0.01744763 -0.00599149  0.00297104 -0.00975568 -0.00459321 -0.00532395\n",
      "   0.00629441 -0.00297463 -0.01108319 -0.0124862 ]\n",
      " [ 0.00434072 -0.00875912  0.00029179 -0.00025822  0.02007207  0.00201143\n",
      "   0.01483384 -0.00189697 -0.00903185 -0.0035333 ]\n",
      " [-0.00934359 -0.00123541 -0.00019838  0.01347546  0.00142322  0.00503818\n",
      "   0.01686059 -0.01164583 -0.01629545  0.00141565]\n",
      " [-0.0037859  -0.02121864  0.00353797 -0.00891872  0.00606827 -0.00641744\n",
      "   0.00198504  0.0088595  -0.01728791 -0.00809459]\n",
      " [-0.00692648  0.01186916 -0.02598371  0.01096163  0.00785588  0.00486047\n",
      "   0.00271539 -0.0114542  -0.00704931  0.01114486]\n",
      " [-0.0107431   0.00198769 -0.01442682  0.01113312 -0.00172008 -0.0016007\n",
      "  -0.01136     0.00691263  0.01343314  0.00195071]\n",
      " [ 0.00860403  0.0094106   0.00287986  0.00550019 -0.00144854 -0.01447581\n",
      "  -0.00839262 -0.01882712  0.00419546 -0.00259434]\n",
      " [-0.00114596 -0.0022411   0.00721215 -0.00399298  0.00385888  0.01368535\n",
      "   0.00014945  0.0094268  -0.00775535  0.00263177]\n",
      " [ 0.02434847 -0.02105992 -0.00542281 -0.00875596  0.01220092 -0.01466742\n",
      "  -0.010826   -0.00705422  0.00771268  0.00642905]]\n",
      "b2 = [[-5.51386209e-08]\n",
      " [ 9.61700208e-09]\n",
      " [ 1.12004772e-07]\n",
      " [ 3.90123975e-08]\n",
      " [-1.64153177e-07]\n",
      " [-5.56547239e-08]\n",
      " [ 5.48167765e-08]\n",
      " [-3.61683432e-08]\n",
      " [ 2.21380267e-08]\n",
      " [-3.36156168e-08]]\n"
     ]
    }
   ],
   "source": [
    "parameters = update_parameters(parameters, grads,alpha=0.003)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model with all function call\n",
    "def model(X, Y, n_h1,n_h2, num_iterations,alpha,lamb, print_cost=False):\n",
    "    parameters =initialize_parameters(X.shape[0],n_h1,n_h2,Y.shape[0])\n",
    "    costs=[]\n",
    "    for i in range(0, num_iterations+1):\n",
    "        A3, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A3,Y,parameters,lamb)\n",
    "        grads = backward_propagation(parameters, cache, X, Y,lamb)\n",
    "        parameters = update_parameters(parameters, grads,alpha)        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            #print (\"Cost after 100 iteration\")\n",
    "            print(\"cost after iteration \"+str(i)+\" is \",cost)\n",
    "    return parameters,costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after iteration 0 is  0.6931498242594425\n",
      "cost after iteration 100 is  0.6931426118986993\n",
      "cost after iteration 200 is  0.6931381081276424\n",
      "cost after iteration 300 is  0.6931347285681302\n",
      "cost after iteration 400 is  0.6931320130972348\n",
      "cost after iteration 500 is  0.6931296822976025\n",
      "cost after iteration 600 is  0.6931275648714167\n",
      "cost after iteration 700 is  0.6931255541752748\n",
      "cost after iteration 800 is  0.6931235821274094\n",
      "cost after iteration 900 is  0.6931216035202774\n",
      "cost after iteration 1000 is  0.6931195865654085\n",
      "cost after iteration 1100 is  0.6931175071691931\n",
      "cost after iteration 1200 is  0.6931153454392384\n",
      "cost after iteration 1300 is  0.693113083520453\n",
      "cost after iteration 1400 is  0.6931107042191271\n",
      "cost after iteration 1500 is  0.6931081900883057\n",
      "cost after iteration 1600 is  0.6931055227763558\n",
      "cost after iteration 1700 is  0.6931026825173354\n",
      "cost after iteration 1800 is  0.6930996476872708\n",
      "cost after iteration 1900 is  0.693096394377086\n",
      "cost after iteration 2000 is  0.6930928959481215\n",
      "cost after iteration 2100 is  0.693089122544299\n",
      "cost after iteration 2200 is  0.6930850405386165\n",
      "cost after iteration 2300 is  0.693080611892302\n",
      "cost after iteration 2400 is  0.6930757934033687\n",
      "cost after iteration 2500 is  0.6930705358179092\n",
      "cost after iteration 2600 is  0.6930647827722697\n",
      "cost after iteration 2700 is  0.6930584695271131\n",
      "cost after iteration 2800 is  0.693051521444914\n",
      "cost after iteration 2900 is  0.6930438521500717\n",
      "cost after iteration 3000 is  0.6930353612946886\n",
      "cost after iteration 3100 is  0.6930259318319952\n",
      "cost after iteration 3200 is  0.6930154266717775\n",
      "cost after iteration 3300 is  0.6930036845557497\n",
      "cost after iteration 3400 is  0.6929905149426008\n",
      "cost after iteration 3500 is  0.6929756916281574\n",
      "cost after iteration 3600 is  0.6929589447399225\n",
      "cost after iteration 3700 is  0.6929399506288489\n",
      "cost after iteration 3800 is  0.6929183190228027\n",
      "cost after iteration 3900 is  0.6928935765887767\n",
      "cost after iteration 4000 is  0.6928651457497425\n",
      "cost after iteration 4100 is  0.6928323171806097\n",
      "cost after iteration 4200 is  0.692794213811995\n",
      "cost after iteration 4300 is  0.6927497433196332\n",
      "cost after iteration 4400 is  0.6926975348501649\n",
      "cost after iteration 4500 is  0.692635853948104\n",
      "cost after iteration 4600 is  0.6925624870261045\n",
      "cost after iteration 4700 is  0.6924745828333977\n",
      "cost after iteration 4800 is  0.6923684325585042\n",
      "cost after iteration 4900 is  0.6922391614042434\n",
      "cost after iteration 5000 is  0.6920802910522429\n",
      "cost after iteration 5100 is  0.691883111841686\n",
      "cost after iteration 5200 is  0.6916357718352876\n",
      "cost after iteration 5300 is  0.6913219414237128\n",
      "cost after iteration 5400 is  0.6909188383680634\n",
      "cost after iteration 5500 is  0.6903942884335755\n",
      "cost after iteration 5600 is  0.6897023416660655\n",
      "cost after iteration 5700 is  0.6887767706734655\n",
      "cost after iteration 5800 is  0.6875216075658702\n",
      "cost after iteration 5900 is  0.685797934612316\n",
      "cost after iteration 6000 is  0.6834069227310032\n",
      "cost after iteration 6100 is  0.6800715133074966\n",
      "cost after iteration 6200 is  0.6754239816973047\n",
      "cost after iteration 6300 is  0.6690119643766324\n",
      "cost after iteration 6400 is  0.6603317178772037\n",
      "cost after iteration 6500 is  0.6488725051357868\n",
      "cost after iteration 6600 is  0.6341402292814559\n",
      "cost after iteration 6700 is  0.6156840840457376\n",
      "cost after iteration 6800 is  0.5967766109498948\n",
      "cost after iteration 6900 is  0.5970604681012172\n",
      "cost after iteration 7000 is  0.5822209800904884\n",
      "cost after iteration 7100 is  0.5642340006709797\n",
      "cost after iteration 7200 is  0.541782981004878\n",
      "cost after iteration 7300 is  0.5611867027780305\n",
      "cost after iteration 7400 is  0.5533535456685511\n",
      "cost after iteration 7500 is  0.5137036832873241\n",
      "cost after iteration 7600 is  0.5125661905904059\n",
      "cost after iteration 7700 is  0.5224290960137515\n",
      "cost after iteration 7800 is  0.5616236840419992\n",
      "cost after iteration 7900 is  0.5491952990286101\n",
      "cost after iteration 8000 is  0.5445392711206879\n",
      "cost after iteration 8100 is  0.5464626049562752\n",
      "cost after iteration 8200 is  0.5257370184083147\n",
      "cost after iteration 8300 is  0.5359524583747852\n",
      "cost after iteration 8400 is  0.5161982043487031\n",
      "cost after iteration 8500 is  0.5211499669215435\n",
      "cost after iteration 8600 is  0.49219227214074673\n",
      "cost after iteration 8700 is  0.5008016748912074\n",
      "cost after iteration 8800 is  0.35216583018048336\n",
      "cost after iteration 8900 is  0.41247041382317967\n",
      "cost after iteration 9000 is  0.48373853824340685\n",
      "cost after iteration 9100 is  0.34838349809083025\n",
      "cost after iteration 9200 is  0.27322228824195466\n",
      "cost after iteration 9300 is  0.2582536588099815\n",
      "cost after iteration 9400 is  0.20360251512728555\n",
      "cost after iteration 9500 is  0.17527456587864584\n",
      "cost after iteration 9600 is  0.25686856330860924\n",
      "cost after iteration 9700 is  0.4937193950156915\n",
      "cost after iteration 9800 is  0.19101175743474944\n",
      "cost after iteration 9900 is  0.29447122122628344\n",
      "cost after iteration 10000 is  0.17710067584853564\n",
      "cost after iteration 10100 is  0.6465278495407024\n",
      "cost after iteration 10200 is  0.1387297604627265\n",
      "cost after iteration 10300 is  0.17009352384854548\n",
      "cost after iteration 10400 is  0.14093510548800367\n",
      "cost after iteration 10500 is  0.1638448334834752\n",
      "cost after iteration 10600 is  0.1068485945375787\n",
      "cost after iteration 10700 is  0.07984758553175596\n",
      "cost after iteration 10800 is  0.06931755089081555\n",
      "cost after iteration 10900 is  0.05546224910674719\n",
      "cost after iteration 11000 is  0.042881790904533534\n",
      "cost after iteration 11100 is  0.7362399795206772\n",
      "cost after iteration 11200 is  0.11194553458131337\n",
      "cost after iteration 11300 is  0.11826813286496358\n",
      "cost after iteration 11400 is  0.034429454085832444\n",
      "cost after iteration 11500 is  0.027131484938349902\n",
      "cost after iteration 11600 is  0.023519216437405736\n",
      "cost after iteration 11700 is  0.020097413788923307\n",
      "cost after iteration 11800 is  0.017422035157636434\n",
      "cost after iteration 11900 is  0.015522981247356824\n",
      "cost after iteration 12000 is  0.014122837625852536\n",
      "cost after iteration 12100 is  0.013000878791556383\n",
      "cost after iteration 12200 is  0.012049329082398648\n",
      "cost after iteration 12300 is  0.011223195593418651\n",
      "cost after iteration 12400 is  0.010498082188038865\n",
      "cost after iteration 12500 is  0.009856492462616492\n",
      "cost after iteration 12600 is  0.00928485952332979\n",
      "cost after iteration 12700 is  0.008772438314747366\n",
      "cost after iteration 12800 is  0.008310592314786942\n",
      "cost after iteration 12900 is  0.007892291599512184\n",
      "cost after iteration 13000 is  0.007511751298093708\n",
      "cost after iteration 13100 is  0.007164165421119747\n",
      "cost after iteration 13200 is  0.006845507168747406\n",
      "cost after iteration 13300 is  0.006552376720796367\n",
      "cost after iteration 13400 is  0.006281883634714981\n",
      "cost after iteration 13500 is  0.006031554899305717\n",
      "cost after iteration 13600 is  0.00579926228580716\n",
      "cost after iteration 13700 is  0.005583164399124643\n",
      "cost after iteration 13800 is  0.005381660054268831\n",
      "cost after iteration 13900 is  0.005193350467055244\n",
      "cost after iteration 14000 is  0.0050170083685950025\n",
      "cost after iteration 14100 is  0.004851552604926223\n",
      "cost after iteration 14200 is  0.004696027116230834\n",
      "cost after iteration 14300 is  0.004549583438403302\n",
      "cost after iteration 14400 is  0.004411466056752553\n",
      "cost after iteration 14500 is  0.004281000083786226\n",
      "cost after iteration 14600 is  0.004157580842039603\n",
      "cost after iteration 14700 is  0.004040665017179643\n",
      "cost after iteration 14800 is  0.003929763112244713\n",
      "cost after iteration 14900 is  0.0038244329853575217\n",
      "cost after iteration 15000 is  0.0037242742938928524\n",
      "cost after iteration 15100 is  0.0036289237003732508\n",
      "cost after iteration 15200 is  0.0035380507211737644\n",
      "cost after iteration 15300 is  0.003451354119857258\n",
      "cost after iteration 15400 is  0.003368558763719397\n",
      "cost after iteration 15500 is  0.0032894128757287215\n",
      "cost after iteration 15600 is  0.0032136856251490974\n",
      "cost after iteration 15700 is  0.003141165009230293\n",
      "cost after iteration 15800 is  0.003071655985842286\n",
      "cost after iteration 15900 is  0.003004978823119623\n",
      "cost after iteration 16000 is  0.0029409676373196433\n",
      "cost after iteration 16100 is  0.0028794690943777287\n",
      "cost after iteration 16200 is  0.0028203412542205314\n",
      "cost after iteration 16300 is  0.0027634525398997937\n",
      "cost after iteration 16400 is  0.002708680816135685\n",
      "cost after iteration 16500 is  0.0026559125639923574\n",
      "cost after iteration 16600 is  0.0026050421402157367\n",
      "cost after iteration 16700 is  0.0025559711112989685\n",
      "cost after iteration 16800 is  0.002508607653649315\n",
      "cost after iteration 16900 is  0.0024628660123483933\n",
      "cost after iteration 17000 is  0.0024186660119548216\n",
      "cost after iteration 17100 is  0.002375932613621446\n",
      "cost after iteration 17200 is  0.00233459551350726\n",
      "cost after iteration 17300 is  0.0022945887780755794\n",
      "cost after iteration 17400 is  0.002255850512399025\n",
      "cost after iteration 17500 is  0.002218322558050322\n",
      "cost after iteration 17600 is  0.0021819502175568395\n",
      "cost after iteration 17700 is  0.0021466820027435415\n",
      "cost after iteration 17800 is  0.0021124694045922413\n",
      "cost after iteration 17900 is  0.0020792666825099047\n",
      "cost after iteration 18000 is  0.0020470306711307352\n",
      "cost after iteration 18100 is  0.0020157206029806584\n",
      "cost after iteration 18200 is  0.0019852979455120944\n",
      "cost after iteration 18300 is  0.0019557262511744945\n",
      "cost after iteration 18400 is  0.0019269710193257985\n",
      "cost after iteration 18500 is  0.001898999568913035\n",
      "cost after iteration 18600 is  0.0018717809209593383\n",
      "cost after iteration 18700 is  0.0018452856899915041\n",
      "cost after iteration 18800 is  0.0018194859836282717\n",
      "cost after iteration 18900 is  0.001794355309625826\n",
      "cost after iteration 19000 is  0.0017698684897454052\n",
      "cost after iteration 19100 is  0.0017460015798685961\n",
      "cost after iteration 19200 is  0.0017227317958404085\n",
      "cost after iteration 19300 is  0.0017000374445688003\n",
      "cost after iteration 19400 is  0.0016778978599529264\n",
      "cost after iteration 19500 is  0.00165629334325167\n",
      "cost after iteration 19600 is  0.0016352051075389853\n",
      "cost after iteration 19700 is  0.0016146152259244072\n",
      "cost after iteration 19800 is  0.0015945065832454514\n",
      "cost after iteration 19900 is  0.0015748628309643715\n",
      "cost after iteration 20000 is  0.0015556683450250052\n",
      "W1 = [[-0.00011903  0.00636859 -0.00324586 ... -0.02685062  0.00067157\n",
      "  -0.0164231 ]\n",
      " [-0.00330959  0.01922119 -0.01123125 ... -0.00324946  0.0136281\n",
      "   0.00494208]\n",
      " [-0.00495291  0.01053941  0.0204995  ...  0.00861625 -0.01317142\n",
      "   0.00530233]\n",
      " ...\n",
      " [-0.0064979  -0.00643954  0.00576795 ...  0.0086662   0.01636308\n",
      "  -0.01361686]\n",
      " [-0.00191455  0.00051389 -0.00079841 ... -0.00243773 -0.00998033\n",
      "   0.00534885]\n",
      " [-0.01992496 -0.01552805  0.00220639 ...  0.00836881  0.00456892\n",
      "   0.01172881]]\n",
      "b1 = [[ 2.08213475e-02]\n",
      " [ 1.37156118e-02]\n",
      " [ 1.76671144e-05]\n",
      " [-8.93524137e-03]\n",
      " [ 8.10925649e-03]\n",
      " [ 8.02271137e-03]\n",
      " [ 9.58564130e-03]\n",
      " [-7.12148092e-03]\n",
      " [ 1.50334534e-02]\n",
      " [ 7.05155887e-03]\n",
      " [ 5.28334228e-03]\n",
      " [-4.16373011e-02]\n",
      " [-1.92137610e-02]\n",
      " [ 1.30248048e-02]\n",
      " [ 5.08654113e-03]\n",
      " [-1.21137260e-03]\n",
      " [ 5.23046979e-03]\n",
      " [ 1.47111170e-03]\n",
      " [ 1.04456035e-02]\n",
      " [ 1.86613152e-02]]\n",
      "W2 = [[-4.45044618e-02 -2.66749164e-02  1.31448686e-02 -1.34668739e-02\n",
      "   4.42650248e-03  1.89595213e-02  2.39900361e-03 -2.98232967e-02\n",
      "   7.05794694e-03 -1.43919271e-02 -4.99433574e-03 -2.19658955e-02\n",
      "   2.19340843e-02  1.30259659e-02 -1.64730508e-02 -4.29735859e-02\n",
      "   1.86672779e-02  1.64201797e-04 -1.71074505e-02 -2.88749278e-03]\n",
      " [-1.26417989e-01 -1.23032760e-01  1.16645086e-01 -1.52375242e-01\n",
      "   3.68762735e-02  3.00856042e-02  3.68192870e-02 -1.38928421e-01\n",
      "   5.07983288e-02 -7.51574745e-02  1.01721651e-02 -1.09087184e-01\n",
      "   1.10560130e-01  5.24509798e-02  3.75835689e-03 -9.29368381e-02\n",
      "   1.25750149e-01  2.51996766e-03 -1.12794116e-01  1.08965387e-01]\n",
      " [ 1.60901952e-01  1.70727962e-01 -1.62184262e-01  1.72567394e-01\n",
      "  -3.29596813e-02 -4.40464447e-02 -4.07976619e-02  1.76088072e-01\n",
      "  -8.18225822e-02  8.76547240e-02 -2.14525432e-02  1.45873235e-01\n",
      "  -1.70931801e-01 -5.44585564e-02  9.33199254e-03  8.80083963e-02\n",
      "  -1.30664875e-01 -1.03543513e-02  1.30214423e-01 -1.10499967e-01]\n",
      " [-8.77716895e-02 -8.54525080e-02  8.83020218e-02 -9.01933564e-02\n",
      "   1.03149679e-02  3.71514081e-03  1.15408561e-02 -7.58204462e-02\n",
      "   4.61109960e-02 -3.97864308e-02  7.86888263e-03 -7.17232386e-02\n",
      "   6.39812685e-02  1.00774714e-02 -1.91746025e-02 -3.95633515e-02\n",
      "   5.60591304e-02 -2.42461396e-02 -6.78573824e-02  5.01429471e-02]\n",
      " [ 8.93723693e-02  8.59807395e-02 -8.68669916e-02  9.23405625e-02\n",
      "  -2.58201342e-02 -2.54710176e-02 -2.18629787e-02  1.00357524e-01\n",
      "  -4.24584850e-02  6.18234707e-02 -1.82058816e-02  7.55691100e-02\n",
      "  -8.33118446e-02 -3.64078194e-02 -7.88541475e-03  5.29516868e-02\n",
      "  -5.63655248e-02 -4.98982896e-04  6.11091169e-02 -5.05147265e-02]\n",
      " [-3.73604573e-01 -3.87564038e-01  3.57677784e-01 -4.17554640e-01\n",
      "   1.01162607e-01  6.86506774e-02  1.12047149e-01 -4.00494053e-01\n",
      "   1.96229285e-01 -1.78870016e-01  2.66952686e-02 -3.28311913e-01\n",
      "   3.51430398e-01  1.41884185e-01 -2.05247873e-02 -2.51365744e-01\n",
      "   2.72364404e-01  8.73400284e-03 -2.94204298e-01  2.83173827e-01]\n",
      " [-3.48914790e-01 -3.75927874e-01  3.46442822e-01 -4.10680326e-01\n",
      "   9.72850203e-02  7.37321588e-02  1.02769413e-01 -3.64175194e-01\n",
      "   1.78317198e-01 -1.84179383e-01  2.63110738e-02 -3.11067529e-01\n",
      "   3.31904422e-01  1.37074489e-01 -3.42918418e-02 -2.22568643e-01\n",
      "   2.53041851e-01  5.10487235e-03 -2.89732872e-01  2.77365010e-01]\n",
      " [ 2.85471400e-01  3.16905029e-01 -2.86084615e-01  3.23681086e-01\n",
      "  -7.30139017e-02 -7.28793182e-02 -8.25573584e-02  2.94105617e-01\n",
      "  -1.48000396e-01  1.59862749e-01 -2.88604652e-02  2.59819350e-01\n",
      "  -2.71237638e-01 -1.16646540e-01  1.80370197e-02  1.82664695e-01\n",
      "  -2.28615789e-01 -1.63262522e-02  2.35826514e-01 -2.33496686e-01]\n",
      " [ 2.27405487e-02  2.16809728e-02 -3.00237427e-02  3.88513375e-02\n",
      "  -1.54670799e-02 -4.84002422e-03 -2.79701233e-02  2.94973735e-02\n",
      "  -2.51347124e-02  2.72353659e-03 -1.20039322e-02  2.52899345e-02\n",
      "  -4.02500941e-02 -1.58899463e-02 -1.13842159e-02  2.50833807e-02\n",
      "  -3.64856331e-02  1.25374274e-02  3.14855454e-02 -1.72908298e-02]\n",
      " [ 2.28287940e-01  2.50456381e-01 -2.37987582e-01  2.47468656e-01\n",
      "  -6.66781455e-02 -4.47308559e-02 -8.89925761e-02  2.55415910e-01\n",
      "  -1.28977345e-01  1.21651002e-01 -2.33320748e-02  1.90636200e-01\n",
      "  -2.29145442e-01 -8.29291397e-02  1.74886319e-02  1.57819615e-01\n",
      "  -1.80908868e-01 -1.18283726e-02  1.84104331e-01 -2.00945036e-01]\n",
      " [ 1.53372760e-01  1.69544709e-01 -1.64844403e-01  1.82721404e-01\n",
      "  -4.92680136e-02 -3.70307805e-02 -3.44134629e-02  1.59285904e-01\n",
      "  -8.58254103e-02  9.22040563e-02 -1.30656623e-02  1.53179830e-01\n",
      "  -1.61524093e-01 -6.28220773e-02  1.57713086e-02  1.16672055e-01\n",
      "  -1.33120761e-01  5.82471740e-05  1.34296740e-01 -1.47980102e-01]\n",
      " [-1.41507463e-01 -1.59786999e-01  1.44025496e-01 -1.76130073e-01\n",
      "   4.43202215e-02  3.97443395e-02  3.74651718e-02 -1.74552729e-01\n",
      "   7.00546512e-02 -1.04566682e-01 -1.06192219e-03 -1.29436777e-01\n",
      "   1.36827096e-01  7.48475569e-02 -3.24021801e-03 -8.99182755e-02\n",
      "   1.31989381e-01  1.71572484e-02 -1.30727558e-01  1.19404982e-01]\n",
      " [ 2.71130669e-01  2.81934061e-01 -2.67685318e-01  3.02378496e-01\n",
      "  -7.80105096e-02 -5.79790981e-02 -8.42026497e-02  2.77005865e-01\n",
      "  -1.36881912e-01  1.42504039e-01 -2.16415652e-02  2.32680656e-01\n",
      "  -2.46929162e-01 -1.08183768e-01  2.36856989e-02  1.65819191e-01\n",
      "  -2.14252918e-01 -1.37638726e-02  2.20487646e-01 -2.24847501e-01]\n",
      " [ 2.29595954e-01  2.53106268e-01 -2.45865835e-01  2.44225704e-01\n",
      "  -2.94038201e-02 -5.63823877e-02 -7.49973906e-02  2.44474540e-01\n",
      "  -1.50023646e-01  1.23106903e-01 -2.52179858e-02  2.13408204e-01\n",
      "  -2.22877872e-01 -8.40372259e-02  2.33689970e-02  1.37679177e-01\n",
      "  -1.85480095e-01  1.75361841e-03  1.85584114e-01 -1.87080088e-01]\n",
      " [ 4.08247762e-02  4.51705527e-02 -3.29646060e-02  4.10264154e-02\n",
      "  -5.43077865e-03 -1.89394658e-02 -2.21898389e-02  4.64474713e-02\n",
      "  -2.64299942e-02  2.69105106e-02  4.60339051e-03  4.16967469e-02\n",
      "  -2.13052606e-02 -2.45648552e-02  1.32437977e-03  1.56141003e-02\n",
      "  -4.41550347e-02  1.59607999e-02  3.43986320e-02 -6.00483245e-02]\n",
      " [-1.36868930e-01 -1.58587749e-01  1.49807462e-01 -1.45537189e-01\n",
      "   3.79623479e-02  2.80173618e-02  5.27638236e-02 -1.60116546e-01\n",
      "   5.93103211e-02 -7.70908754e-02 -1.47127806e-02 -1.27258199e-01\n",
      "   1.15512426e-01  4.83863178e-02 -2.38376617e-02 -8.34334063e-02\n",
      "   1.14254076e-01  1.05648753e-02 -1.14379977e-01  9.28672875e-02]\n",
      " [-2.60728228e-01 -2.79434748e-01  2.47021448e-01 -2.86059505e-01\n",
      "   4.56138165e-02  6.50719268e-02  8.13417902e-02 -2.75159486e-01\n",
      "   1.36322926e-01 -1.36903729e-01  1.42569126e-02 -2.24656283e-01\n",
      "   2.42548239e-01  9.07201935e-02 -2.12346024e-02 -1.59351000e-01\n",
      "   2.09843736e-01  2.35194563e-03 -2.07474416e-01  2.01869636e-01]\n",
      " [ 1.43086555e-01  1.46630655e-01 -1.62710393e-01  1.73586185e-01\n",
      "  -3.55493726e-02 -4.39220047e-02 -4.66173117e-02  1.54963161e-01\n",
      "  -7.13380886e-02  7.13223016e-02 -2.76606321e-02  1.31982588e-01\n",
      "  -1.53222334e-01 -6.37757906e-02  7.50274802e-03  8.60236439e-02\n",
      "  -1.34167729e-01 -1.91623446e-02  1.10082298e-01 -1.25942869e-01]\n",
      " [ 2.26337367e-01  2.51431543e-01 -2.39420745e-01  2.61919940e-01\n",
      "  -6.10976531e-02 -4.39569331e-02 -7.01995712e-02  2.44633754e-01\n",
      "  -1.27272568e-01  1.19260537e-01 -1.92864553e-02  2.05444061e-01\n",
      "  -2.27379547e-01 -9.61157806e-02  3.60907887e-03  1.45833239e-01\n",
      "  -1.93545919e-01 -4.89445252e-03  1.98165354e-01 -1.78775572e-01]\n",
      " [ 1.57009804e-01  1.55697923e-01 -1.67427061e-01  1.79207712e-01\n",
      "  -3.36511231e-02 -2.87531410e-02 -7.04870721e-02  1.73387943e-01\n",
      "  -8.05639611e-02  6.38078648e-02 -1.28275075e-02  1.28220071e-01\n",
      "  -1.59994486e-01 -6.54090309e-02  6.48289081e-03  9.72257340e-02\n",
      "  -1.39434899e-01 -4.58486148e-03  1.31751654e-01 -1.41111656e-01]]\n",
      "b2 = [[ 0.0007416 ]\n",
      " [ 0.00058926]\n",
      " [ 0.00540799]\n",
      " [ 0.00035989]\n",
      " [ 0.00302158]\n",
      " [-0.00987958]\n",
      " [-0.0111881 ]\n",
      " [ 0.00587539]\n",
      " [ 0.00060609]\n",
      " [ 0.00275365]\n",
      " [ 0.00404794]\n",
      " [-0.0001134 ]\n",
      " [ 0.01040767]\n",
      " [ 0.00056589]\n",
      " [ 0.00137913]\n",
      " [ 0.00187998]\n",
      " [-0.00019601]\n",
      " [ 0.0023007 ]\n",
      " [ 0.00222779]\n",
      " [ 0.00358291]]\n",
      "W3 = [[-0.13778412 -0.48197516  0.46336791 -0.30609534  0.20957764 -1.39703357\n",
      "  -1.32012245  0.9399269   0.03948318  0.72765243  0.47903025 -0.5642509\n",
      "   0.8567539   0.72483013  0.07535329 -0.5131549  -0.9381541   0.42995291\n",
      "   0.72963504  0.46367625]]\n",
      "b3 = [[0.04914929]]\n"
     ]
    }
   ],
   "source": [
    "parameters,costs = model(X_train, Y_train, 20,20, num_iterations=20000,alpha=0.01,lamb=0.7, print_cost=True)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "print(\"b3 = \" + str(parameters[\"b3\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x265078a0470>]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting cost versus iterations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for predicting user input image\n",
    "def predict(parameters, X):\n",
    "    A2, cache = forward_propagation(X,parameters)\n",
    "    predictions = A2    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1=predict(parameters,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.844753%\n"
     ]
    }
   ],
   "source": [
    "#printing accuracy \n",
    "print ('Accuracy: %f' % float((np.dot(Y_train,predictions1.T) + np.dot(1-Y_train,1-predictions1.T))/float(Y_train.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 1)\n"
     ]
    }
   ],
   "source": [
    "# reading user input image and reshapeing it into (64,64) size\n",
    "img=cv2.imread(\"C:/Users/nagarjun/Desktop/datset/trail(cat)/cat.292.jpg\")\n",
    "resized_image=cv2.resize(img,(64,64))\n",
    "X=np.array(resized_image)\n",
    "X=X.reshape((64*64*3,1))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=predict(parameters,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01077352]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if pred>0.5:\n",
    "    print(\"It's a cat\")\n",
    "else:\n",
    "    print(\"It's not a cat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
